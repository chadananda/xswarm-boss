# Voice Assistant - Moshi + ThinkingEngine

A complete voice-first personal assistant that uses **Moshi MLX** as the real-time voice interface, enhanced with **personality**, **memory**, **thinking**, and **tools** to create a full-featured AI assistant.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TUI Dashboard                            â”‚
â”‚  (Textual app with visualizer, chat, activity feed)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚
          â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Voice Server   â”‚    â”‚   Thinking Engine       â”‚
â”‚  (ZeroMQ daemon)â”‚    â”‚   (Claude Haiku/Sonnet) â”‚
â”‚                 â”‚    â”‚                         â”‚
â”‚  â€¢ Moshi MLX    â”‚â—„â”€â”€â”€â”¤  â€¢ Decision making      â”‚
â”‚  â€¢ Audio I/O    â”‚    â”‚  â€¢ Memory search        â”‚
â”‚  â€¢ Transcript   â”‚â”€â”€â”€â”€â–º  â€¢ Tool execution       â”‚
â”‚  â€¢ Context      â”‚    â”‚  â€¢ Context injection    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚  Tool Registry  â”‚
         â”‚              â”‚  â€¢ Email        â”‚
         â”‚              â”‚  â€¢ Phone        â”‚
         â”‚              â”‚  â€¢ Memory       â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   User    â”‚
   â”‚  (voice)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Moshi** handles real-time voice conversation (listen + speak simultaneously)
2. **ThinkingEngine** monitors both user input and Moshi's output
3. When action is needed, **Claude Haiku** decides what to do (search memory, call tool, inject context)
4. **Claude Sonnet 4.5** summarizes results tersely for Moshi's small ~3000 token context
5. Context is injected back into Moshi to inform its responses

### Key Components

- **Voice Server** (`voice_server.py`): ZeroMQ daemon running Moshi MLX in separate process for Metal GPU
- **ThinkingEngine** (`thinking_engine.py`): Background system monitoring conversations, deciding on actions
- **Tool Registry** (`tools/`): Email, phone, memory search, theme changes
- **Persona System** (`personas/`): YAML-based personality configs with traits and system prompts
- **Memory Manager** (`memory.py`): Conversation history with server fallback

---

## Quick Start

### Installation

```bash
cd packages/assistant

# Install Python dependencies
pip install -r requirements.txt

# Or install in development mode
pip install -e ".[dev]"

# Download Vosk model for wake word detection
python scripts/download_vosk_model.py

# Install MOSHI from source (required for Phase 2)
cd /tmp
git clone https://github.com/kyutai-labs/moshi.git moshi-official
cd moshi-official/moshi
pip install -e .
cd -
```

### Setup Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your settings
# XSWARM_SERVER_URL=http://localhost:3000
# XSWARM_API_TOKEN=your-token
```

### Run the Assistant

```bash
# Run with default settings (launches interactive TUI)
python -m assistant.main

# Or use the CLI entry point (after pip install)
assistant

# Or the full name
voice-assistant

# First run will show setup wizard
assistant

# Run with debug logging
assistant --debug

# Use custom config file
assistant --config /path/to/config.yaml
```

## Interactive TUI Interface

![Voice Assistant TUI Dashboard](docs/tui-screenshot.svg)

The assistant is a **fully interactive TUI application** - all configuration happens inside the interface, not via command-line flags.

### First-Run Setup Wizard

On first launch, you'll see a welcome wizard that guides you through:

1. **Persona Selection** - Choose your assistant's personality
2. **Device Selection** - Auto-detect or manually select compute device (MPS, CUDA, CPU)
3. **Wake Word** - Set your activation phrase (e.g., "jarvis", "computer")
4. **Memory Server** - Configure memory server connection (optional)

Configuration is saved to `~/.config/xswarm/config.yaml` and persists between runs.

### Keyboard Controls

Once running, the TUI provides these keyboard shortcuts:

- **`s`** - Open settings (change any configuration)
- **`SPACE`** - Toggle listening mode
- **`q`** - Quit application

### Settings Screen

Press `s` at any time to open the interactive settings screen where you can modify:

- Active persona
- Compute device (auto/mps/cuda/cpu)
- Wake word phrase
- Memory server URL
- API token
- Enable/disable memory integration

All changes are saved immediately to your config file.

## Command Line Options

Only development and testing flags are available:

```
usage: assistant [-h] [--config CONFIG] [--debug] [--version]

Voice Assistant with MOSHI - Interactive TUI

optional arguments:
  -h, --help            show this help message and exit
  --config CONFIG       Path to custom config file
  --debug               Enable debug logging
  --version             show program's version number and exit

Examples:
  assistant                    # Launch interactive TUI
  assistant --debug            # Launch with debug logging
  assistant --config /path     # Use custom config file

Configuration:
  All settings are configured interactively in the TUI.
  Press 's' inside the app to open settings.
  Config saved to: ~/.config/xswarm/config.yaml
```

## Testing the TUI

### Overview

All TUI testing runs in **headless mode** (no terminal corruption!). Perfect for automated testing and AI collaboration.

### Install Test Dependencies

```bash
pip install -e ".[dev]"
```

This installs:
- `pytest` - Test framework
- `pytest-asyncio` - Async test support
- `pytest-cov` - Coverage reporting
- **`pytest-textual-snapshot`** â­ - Visual snapshot testing

### Run Tests

```bash
# Run all tests
pytest tests/ -v

# Run snapshot tests (visual regression testing)
pytest tests/test_*_snapshots.py -v

# Run specific test file
pytest tests/test_integration.py -v
pytest tests/test_chat_panel_snapshots.py -v

# Run with coverage
pytest tests/ --cov=assistant --cov-report=html

# Update visual baselines (after intentional UI changes)
pytest tests/test_*_snapshots.py --snapshot-update
```

### Generate SVG Screenshots for AI Review

Perfect for verifying visual changes without running the full app:

```bash
# Generate all component screenshots (headless, no terminal corruption)
python scripts/generate_test_svgs.py

# Generate specific components
python scripts/generate_test_svgs.py --component chat
python scripts/generate_test_svgs.py --component voice

# Custom terminal size
python scripts/generate_test_svgs.py --size 120x40

# See all options
python scripts/generate_test_svgs.py --help
```

Output: `tmp/ai_review/*.svg` (open in browser to view)

### Visual Snapshot Testing

The snapshot tests automatically detect visual regressions by comparing current output against baseline snapshots:

```bash
# Run snapshot tests
pytest tests/test_chat_panel_snapshots.py -v

# If visual changed unintentionally:
#   â†’ Review HTML diff report: tests/__snapshots__/report.html
#   â†’ Fix the bug
#   â†’ Re-run tests

# If visual changed intentionally:
pytest tests/test_chat_panel_snapshots.py --snapshot-update
```

**Key Benefits:**
- âœ… No terminal corruption (runs in headless mode)
- âœ… Automated visual regression detection
- âœ… AI can generate and review screenshots
- âœ… Fast feedback loop (seconds)

### Documentation

For detailed testing workflows, best practices, and troubleshooting, see:

ğŸ“– **[docs/testing-guide.md](docs/testing-guide.md)**

**Topics covered:**
- Headless testing architecture
- Writing new snapshot tests
- AI collaboration workflow
- Continuous integration setup
- Troubleshooting common issues

## ğŸš€ Planned Features (Free Forever)

xswarm aims to be the most addictive, customizable, and viral AI assistant TUI. Here's our ambitious roadmap for the **free base version**:

### ğŸ¨ Ultimate Customization & "Ricing"
- **Plugin System** - Dynamic plugin loading, hot-reload, community marketplace
- **Theme Gallery** - Browse, download, and share custom themes with one click
- **Pywal Integration** - Auto-sync colors with your wallpaper (r/unixporn approved!)
- **Persona Themes** - Each AI personality has unique colors, ASCII art, and style
- **Live Theme Preview** - See changes in real-time before applying
- **System Color Sync** - Adapt to Omarchy, macOS, GTK, Windows themes automatically

### ğŸ” Productivity Powerhouse
- **File Search** - Lightning-fast fuzzy file search with preview (replaces `find` + `grep`)
- **Note Search** - Full-text + semantic search across all your documents (Whoosh + ChromaDB)
- **Task Manager** - Beautiful Kanban board widget, SQLite backend, voice-to-task
- **Quick Actions** - Clipboard manager, screenshots, text snippets
- **Web Research** - Built-in scraper for articles, prices, data extraction

### ğŸ¯ Voice & AI Features
- **Voice Commands** - Natural language control of all features
- **Multiple Personas** - JARVIS (professional), GLaDOS (sarcastic), NEON (cyberpunk), and more
- **Custom Personas** - Create and share your own AI personalities
- **Local Transcription** - Offline meeting transcription with Whisper (100% private)
- **Voice-to-Task** - "Add task: fix bug" instantly creates todo item

### ğŸ“Š System Integration
- **System Monitor** - Live CPU/GPU/memory/network stats in TUI
- **Device Indicators** - Battery, temperature, disk usage
- **Process Manager** - Kill processes, monitor resource hogs
- **Clipboard History** - Never lose a copy again

### ğŸ­ Community & Viral Features
- **Rice Showcase** - Share your setup with screenshots + theme files
- **Plugin Marketplace** - Discover and install community plugins
- **Theme Rating** - Upvote favorite themes, "Setup of the Week"
- **One-Click Install** - "Share my rice" generates shareable setup
- **Plugin Developer Tools** - Easy plugin creation with templates

### ğŸ” Privacy-First Design
- **100% Local** - All base features work offline
- **No Telemetry** - Zero tracking or analytics
- **Open Source** - Fully auditable code
- **Self-Hosted** - Your data never leaves your machine
- **E2E Encryption** - Optional cloud sync with end-to-end encryption

### ğŸ¨ Visual Polish
- **Responsive TUI** - Works at any terminal size (40x15 to 4K)
- **Progressive Degradation** - Graceful feature reduction on small screens
- **60 FPS Animations** - Smooth, buttery animations
- **ASCII Art** - Persona avatars with character
- **Matrix Rain** - Optional cyberpunk background effect
- **Glow Effects** - Neon-style text glowing

### ğŸ”Œ Built-in Plugins (Free)
All these plugins ship with the base install:

1. **file-search** - Fuzzy file finder with preview
2. **note-search** - Full-text note search
3. **task-manager** - Kanban board for todos
4. **system-info** - Live system stats
5. **quick-actions** - Clipboard, screenshots, snippets
6. **web-research** - Article scraper and summarizer
7. **theme-gallery** - Browse and install community themes
8. **persona-manager** - Switch AI personalities

### ğŸ’ Premium Features (Optional, BYOK)
These are **optional** for power users. Base version is amazing without them:

- **Email Suite** - Gmail API integration (bring your own API key)
- **Calendar Sync** - Google Calendar sync (BYOK)
- **Cloud Backup** - E2E encrypted cloud sync (optional paid service)
- **Premium AI Models** - GPT-4, Claude API (BYOK)
- **Workflow Automation** - Advanced task scheduling (APScheduler/Celery)
- **Team Features** - Shared plugins, themes, workspaces

**Philosophy:** Give away the sizzle, sell the steak. Base version is so good users will want to share it.

---

## What's Implemented

### Core: Voice Server (ZeroMQ + Moshi MLX) âœ…
- âœ… ZeroMQ daemon for Moshi MLX (`assistant/voice_server.py`)
- âœ… Separate process for Metal GPU utilization
- âœ… Three-port architecture (commands, audio in, audio out)
- âœ… Real-time duplex audio streaming
- âœ… Transcript monitoring with polling API
- âœ… Context injection for persona/memory/tools
- âœ… VoiceServerClient for easy integration

### Core: Thinking Engine âœ…
- âœ… ThinkingEngine class (`assistant/thinking_engine.py`)
- âœ… Two-step architecture: Haiku (decision) + Sonnet (summarization)
- âœ… Monitors user input and Moshi output
- âœ… Automatic memory search when relevant
- âœ… Tool execution with result injection
- âœ… Terse summarization for Moshi's small context
- âœ… Callbacks for UI notifications

### Core: Tool System âœ…
- âœ… ToolRegistry with JSON schema generation (`assistant/tools/registry.py`)
- âœ… Email tool via SendGrid (`assistant/tools/email_tool.py`)
- âœ… Phone tool via Twilio (`assistant/tools/phone_tool.py`)
- âœ… Theme change tool (`assistant/tools/theme_tool.py`)
- âœ… Memory search tool (built into ThinkingEngine)
- âœ… Async handlers with structured results

### Dashboard: Textual TUI âœ…
- âœ… Main TUI application (`assistant/dashboard/app.py`)
- âœ… Voice visualizer with dual circles (mic/output)
- âœ… Activity feed with timestamps
- âœ… Chat panel with history
- âœ… Settings screen
- âœ… Multiple tab views (status, settings, tools, chat, projects, schedule, workers)
- âœ… Persona theme integration
- âœ… 30 FPS smooth animations

### Persona System âœ…
- âœ… PersonaConfig with Pydantic models (`assistant/personas/config.py`)
- âœ… PersonaManager for loading/switching (`assistant/personas/manager.py`)
- âœ… Big Five personality traits + custom dimensions
- âœ… External YAML configuration
- âœ… System prompt generation from traits
- âœ… Theme colors per persona

### Memory Integration âœ…
- âœ… MemoryManager with server fallback (`assistant/memory.py`)
- âœ… Conversation history storage
- âœ… Context retrieval for LLM
- âœ… Local cache for offline operation

### Wake Word Detection âœ…
- âœ… Vosk-based offline detection (`assistant/wake_word/detector.py`)
- âœ… Multiple wake words (all persona names + common phrases)
- âœ… <100ms latency, deterministic

### Infrastructure âœ…
- âœ… Hardware detection (GPU type, memory)
- âœ… Service selection based on capabilities
- âœ… Singleton lock for single instance
- âœ… Graceful shutdown

## Architecture

### Voice Backend: Moshi MLX
- **Mac M-series**: Moshi MLX on Metal GPU (primary)
- Runs in separate process for GPU isolation
- Quantized models: Q4 (fast), Q8 (balanced), BF16 (quality)
- Real-time full-duplex audio (listen + speak simultaneously)

### AI Models
- **Moshi**: Voice conversation (MLX, local)
- **Claude Haiku**: Quick decisions in ThinkingEngine
- **Claude Sonnet 4.5**: Smart summarization for context injection

### TUI Framework: Textual âœ…
- Modern async/await
- Voice visualizer with dual amplitude circles
- Real-time activity feed
- 30 FPS animations
- Multi-tab navigation

### Persona System: External YAML configs âœ…
- Directory-based (`packages/personas/`)
- Hot-reloadable
- Pydantic models for validation
- Big Five + custom personality traits
- Theme colors and system prompts

### Tool System âœ…
- Registry with JSON schema for LLM
- Email (SendGrid), Phone (Twilio)
- Memory search
- Extensible async handlers

### Wake Word Detection: Vosk âœ…
- Offline (no API calls)
- Lightweight (~40MB model)
- <100ms latency
- Multiple wake words per session

### Memory Integration âœ…
- Async httpx client for Node.js server
- Automatic fallback to local cache
- Conversation history for ThinkingEngine

---

## Voice Server (ZeroMQ)

The voice server runs Moshi MLX in a separate process for proper Metal GPU utilization. Communication happens over ZeroMQ sockets.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     ZeroMQ      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TUI App   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Voice Server â”‚
â”‚             â”‚   Port 5555     â”‚              â”‚
â”‚ VoiceServer â”‚   (commands)    â”‚  Moshi MLX   â”‚
â”‚   Client    â”‚                 â”‚  Audio I/O   â”‚
â”‚             â”‚   Port 5556     â”‚  Transcript  â”‚
â”‚             â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚              â”‚
â”‚             â”‚   (audio in)    â”‚              â”‚
â”‚             â”‚                 â”‚              â”‚
â”‚             â”‚   Port 5557     â”‚              â”‚
â”‚             â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (audio out)   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Server API

The voice server exposes these commands via ZeroMQ:

```python
from assistant.voice_server import VoiceServerClient

client = VoiceServerClient()

# Persona & Context
client.set_persona(name, system_prompt, traits)
client.inject_context("User prefers dark themes")
client.inject_tool_result("email", "Sent to chad@example.com")

# Conversation History
client.get_history()
client.inject_history([...])
client.clear_history()

# Transcript Monitoring
client.get_transcript()      # Full conversation
client.get_new_text()        # New text since last call (for polling)
client.clear_transcript()

# Context Management
client.get_context_usage()   # Track ~3000 token budget

# Audio Streaming
client.send_audio(samples)   # Mic input to Moshi
client.recv_audio()          # Moshi output + amplitudes
```

### Starting the Server

The server is started automatically by `main.py` before launching the TUI:

```python
from assistant.voice_server import start_server_process

# Start in separate process
process = start_server_process(quality="q4")  # or "q8", "bf16"

# Server runs on localhost:5555 (commands), 5556 (audio in), 5557 (audio out)
```

---

## Thinking Engine

The ThinkingEngine monitors conversations and decides when to use tools, search memory, or inject context.

### Two-Step Architecture

1. **Decision (Claude Haiku)** - Fast, cheap
   - Analyzes recent conversation
   - Decides: search memory? execute tool? inject context? do nothing?

2. **Summarization (Claude Sonnet 4.5)** - Smart, quality
   - Takes raw data from tools/memory
   - Creates terse 2-3 sentence summaries
   - Max 150 tokens to fit Moshi's ~3000 token context

### How It Works

```python
from assistant.thinking_engine import ThinkingEngine

engine = ThinkingEngine(
    voice_client=client,
    memory_manager=memory,
    user_id="user-123"
)

# Callbacks for UI updates
engine.on_injection = lambda ctx: print(f"Injected: {ctx}")
engine.on_tool_result = lambda name, result: print(f"Tool: {name}")

await engine.start()

# Engine now monitors:
# - User input (via process_user_input())
# - Moshi output (via polling get_new_text())
```

### Available Tools

The ThinkingEngine can execute these tools:

- **send_email** - Send email via SendGrid
- **make_call** - Make phone call via Twilio
- **search_memory** - Search conversation history
- **change_theme** - Switch TUI color theme

### Example Flow

```
User: "Email me the status update"
         â”‚
         â–¼
ThinkingEngine (Haiku): "action: tool_call, tool: send_email"
         â”‚
         â–¼
ToolRegistry: execute send_email(subject, content)
         â”‚
         â–¼
ThinkingEngine (Sonnet): Summarize result
         â”‚
         â–¼
"Email sent to chad@example.com. Subject: Status Update"
         â”‚
         â–¼
voice_client.inject_tool_result("email", summary)
         â”‚
         â–¼
Moshi: "I've sent you that status update email."
```

---

## Memory Integration

The assistant integrates with the Node.js memory server for persistent conversation history and semantic search.

### Setup

1. Start the memory server:
   ```bash
   cd packages/server
   npm install
   npm start
   ```

2. Configure connection in `.env`:
   ```bash
   cp .env.example .env
   # Edit .env with your API token
   ```

3. Test memory client:
   ```bash
   python examples/test_memory.py
   ```

### Usage

```python
from assistant.memory import MemoryManager

# Initialize with automatic fallback
manager = MemoryManager(
    server_url="http://localhost:3000",
    api_token=os.getenv("XSWARM_API_TOKEN")
)

await manager.initialize()

# Store conversation
await manager.store_message(
    user_id="user-123",
    message="Hello!",
    role="user"
)

# Retrieve context
context = await manager.get_context(
    user_id="user-123",
    query="recent conversations",
    limit=10
)

# Close when done
await manager.close()
```

### Offline Mode

When the memory server is unavailable, the client automatically falls back to a local in-memory cache. This ensures the assistant continues to function even without network connectivity.

**Features:**
- Automatic server health checks
- Graceful fallback to local cache
- Transparent API (same calls work offline)
- 100-message local history buffer

### Memory Client API

**MemoryClient** - Low-level HTTP client:
- `store_message()` - Store conversation message
- `retrieve_context()` - Get relevant context
- `get_conversation_history()` - Get recent history
- `clear_history()` - Clear user history
- `semantic_search()` - Semantic memory search
- `get_preferences()` - Get user preferences
- `set_preference()` - Set user preference
- `health_check()` - Check server health

**MemoryManager** - High-level manager with fallback:
- `initialize()` - Check server and initialize
- `store_message()` - Store with automatic fallback
- `get_context()` - Retrieve with automatic fallback
- `close()` - Close connections

**LocalMemoryCache** - Offline cache:
- `store_message()` - Store locally
- `get_history()` - Get local history
- `clear_history()` - Clear local history

---

## Wake Word Detection

Wake word detection uses [Vosk](https://alphacephei.com/vosk/) for offline, deterministic speech recognition.

### Why Vosk?

- **Offline**: No API calls, fully local
- **Lightweight**: ~40MB model
- **Deterministic**: No AI hallucinations or false positives
- **Low latency**: <100ms detection time
- **No GPU**: Runs on CPU

### Setup

1. Download Vosk model:
   ```bash
   python scripts/download_vosk_model.py
   ```

2. Test wake word detection:
   ```bash
   python examples/test_wake_word.py
   ```

3. Speak "jarvis" into your microphone

### Custom Wake Words

Each persona can have a custom wake word (defined in `packages/personas/persona-name/theme.yaml`):

```yaml
wake_word: "computer"  # Star Trek style
# or
wake_word: "hey assistant"  # Multi-word
```

### Usage

```python
from assistant.wake_word import WakeWordDetector
from pathlib import Path

detector = WakeWordDetector(
    model_path=Path.home() / ".cache" / "vosk" / "vosk-model-small-en-us-0.15",
    wake_word="jarvis",
    sensitivity=0.7
)

def on_wake_word():
    print("Wake word detected!")

detector.start(callback=on_wake_word)

# Process audio frames
detector.process_audio(audio_frame)
```

### With VAD (Voice Activity Detection)

For improved efficiency, use `WakeWordDetectorWithVAD` to only process audio when speech is detected:

```python
from assistant.wake_word import WakeWordDetectorWithVAD

detector = WakeWordDetectorWithVAD(
    model_path=model_path,
    wake_word="jarvis",
    sensitivity=0.7,
    vad_threshold=0.02  # Energy threshold for VAD
)

detector.start(callback=on_wake_word)
detector.process_audio(audio_frame)  # VAD automatically filters
```

---

## Using Personas

Personas are external YAML configurations stored in `packages/personas/`. They are NOT hardcoded in the application.

### Persona Structure

```
packages/personas/
â”œâ”€â”€ jarvis/                 # Example persona (testing only)
â”‚   â”œâ”€â”€ theme.yaml         # Main configuration
â”‚   â”œâ”€â”€ personality.md     # Detailed personality guide
â”‚   â””â”€â”€ vocabulary.yaml    # Vocabulary preferences
â”œâ”€â”€ your-persona/
â”‚   â””â”€â”€ theme.yaml
â””â”€â”€ another-persona/
    â””â”€â”€ theme.yaml
```

### Loading Personas

```python
from assistant.personas import PersonaManager
from pathlib import Path

# Initialize manager
personas_dir = Path(__file__).parent.parent / "personas"
manager = PersonaManager(personas_dir)

# List available personas
print(manager.list_personas())  # ['JARVIS', ...]

# Set active persona
manager.set_current_persona("JARVIS")

# Get system prompt
persona = manager.current_persona
prompt = persona.build_system_prompt()
```

### Creating Your Own Persona

1. Create directory in `packages/personas/your-persona-name/`
2. Create `theme.yaml` with persona configuration
3. Optionally add `personality.md` for detailed guide
4. Optionally add `vocabulary.yaml` for vocabulary preferences
5. Personas are auto-discovered on startup

### Example theme.yaml

```yaml
name: "Your Persona"
description: "Brief description"
version: "1.0.0"

system_prompt: |
  You are a helpful assistant...

traits:
  # Big Five (0.0 - 1.0)
  openness: 0.75
  conscientiousness: 0.85
  extraversion: 0.50
  agreeableness: 0.70
  neuroticism: 0.20

  # Custom dimensions
  formality: 0.75
  enthusiasm: 0.60
  humor: 0.40
  verbosity: 0.50

voice:
  pitch: 1.0
  speed: 1.0
  tone: "neutral"
  quality: 0.8

wake_word: "assistant"
```

---

## Project Structure

```
packages/assistant/
â”œâ”€â”€ assistant/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # âœ… Phase 7 - Main entry point
â”‚   â”œâ”€â”€ config.py                    # âœ… Device detection + memory config
â”‚   â”œâ”€â”€ dashboard/                   # âœ… Phase 3 - Textual TUI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py                   # Main TUI app
â”‚   â”‚   â”œâ”€â”€ styles.tcss              # Textual CSS
â”‚   â”‚   â””â”€â”€ widgets/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ visualizer.py        # Pulsing circle â­
â”‚   â”‚       â”œâ”€â”€ status.py            # Status display
â”‚   â”‚       â””â”€â”€ activity_feed.py     # Activity log
â”‚   â”œâ”€â”€ voice/                       # âœ… Phase 2 - MOSHI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ moshi_pytorch.py         # MOSHI bridge
â”‚   â”‚   â”œâ”€â”€ audio_io.py              # sounddevice I/O
â”‚   â”‚   â””â”€â”€ vad.py                   # Voice Activity Detection
â”‚   â”œâ”€â”€ personas/                    # âœ… Phase 4
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                # PersonaConfig models
â”‚   â”‚   â””â”€â”€ manager.py               # PersonaManager
â”‚   â”œâ”€â”€ wake_word/                   # âœ… Phase 5
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ detector.py              # Vosk detector
â”‚   â””â”€â”€ memory/                      # âœ… Phase 6
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ client.py                # HTTP client + cache
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ test_dashboard.py            # âœ… Dashboard test
â”‚   â”œâ”€â”€ test_personas.py             # âœ… Persona test
â”‚   â”œâ”€â”€ test_wake_word.py            # âœ… Wake word test
â”‚   â””â”€â”€ test_memory.py               # âœ… Memory client test
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_vosk_model.py       # âœ… Model downloader
â”œâ”€â”€ tests/                           # âœ… Phase 7
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_integration.py          # Integration tests
â”‚   â””â”€â”€ test_dashboard.py            # Dashboard widget tests
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ phase3-dashboard-implementation.md  # âœ… Phase 3 docs
â”œâ”€â”€ .env.example                     # âœ… Environment template
â”œâ”€â”€ pyproject.toml                   # âœ… Dependencies + CLI entry points
â”œâ”€â”€ requirements.txt                 # âœ… Pip requirements
â””â”€â”€ README.md                        # This file

packages/personas/                   # âœ… External personas
â”œâ”€â”€ jarvis/                          # Example (testing only)
â”‚   â”œâ”€â”€ theme.yaml
â”‚   â”œâ”€â”€ personality.md
â”‚   â””â”€â”€ vocabulary.yaml
â””â”€â”€ your-persona/                    # Add your own!
    â””â”€â”€ theme.yaml
```

---

## Performance

**Dashboard (Phase 3)**:
- CPU: ~2-5% (Textual is efficient)
- Memory: ~50MB
- Frame rate: Solid 30 FPS
- Latency: <1ms (amplitude â†’ visual)

**Persona System (Phase 4)**:
- Load time: <100ms per persona
- Memory: ~5MB per loaded persona
- Hot-reload: <50ms
- Zero runtime overhead

**Wake Word Detection (Phase 5)**:
- CPU: ~3-8% (single core)
- Memory: ~60MB (model loaded)
- Latency: <100ms (detection)
- Accuracy: >95% (clean audio)
- False positives: <1% (deterministic)

**Memory Integration (Phase 6)**:
- HTTP request latency: ~50-200ms (local server)
- Memory overhead: ~10MB (httpx client)
- Local cache: <5MB (100 messages)
- Fallback time: <100ms (health check)

**Terminal Compatibility**:
- âœ… macOS Terminal
- âœ… iTerm2 (best experience)
- âœ… VSCode integrated terminal
- âœ… Linux terminals with Unicode
- âœ… Windows Terminal (Windows 10+)

---

## Development

### Code Style

```bash
# Format code with Black
black assistant/ tests/

# Type checking with mypy
mypy assistant/

# Run linter
pylint assistant/
```

### Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=assistant --cov-report=html

# Run specific test class
pytest tests/test_integration.py::TestPersonaIntegration -v

# Run specific test method
pytest tests/test_integration.py::TestPersonaIntegration::test_persona_loading -v
```

---

## Troubleshooting

### MOSHI Not Found

If you get `ModuleNotFoundError: No module named 'moshi'`:

```bash
# Install MOSHI from source
cd /tmp
git clone https://github.com/kyutai-labs/moshi.git moshi-official
cd moshi-official/moshi
pip install -e .
```

### Vosk Model Missing

If wake word detection fails:

```bash
# Download the model
python scripts/download_vosk_model.py

# Or download manually from:
# https://alphacephei.com/vosk/models
# Extract to: ~/.cache/vosk/vosk-model-small-en-us-0.15
```

### Memory Server Connection Failed

If memory tests fail:

```bash
# Start the server first
cd packages/server
npm install
npm start

# Or disable memory for testing
assistant --no-memory
```

### Device Detection Issues

If PyTorch device detection fails:

```bash
# Test device detection
python -c "from assistant.config import Config; print(Config().detect_device())"

# Force specific device
assistant --device cpu   # Use CPU
assistant --device mps   # Use Mac Metal
assistant --device cuda  # Use NVIDIA/AMD
```

---

## Current Status

**Version**: 0.3.90

**Core Systems**:
- âœ… ZeroMQ voice server with Moshi MLX
- âœ… ThinkingEngine with Haiku/Sonnet two-step
- âœ… Tool system (email, phone, memory)
- âœ… Textual TUI dashboard
- âœ… Persona system
- âœ… Memory integration

**In Progress**:
- ğŸ”„ End-to-end testing with real Moshi models
- ğŸ”„ Additional tools (calendar, reminders)

---

**Run the assistant:**

```bash
cd packages/assistant
pip install -e .
assistant
```

The voice server starts automatically, ThinkingEngine monitors the conversation, and tools are executed as needed.
